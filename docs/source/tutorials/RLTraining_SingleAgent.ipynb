{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006a11b1",
   "metadata": {},
   "source": [
    "# Single-Agent RL training and deployment\n",
    "\n",
    "The following notebook provides an introduction to training a single reinforcement learning (RL) agent with the proximal policy optimization (PPO) algorithm. We use CommonPower to create a simulation of a power system, within which one node (corresponding to a multi-family household) is controlled by the RL agent. Since RL does not naturally allow considering constraints, such as a minimum state of charge of a battery, we have implemented a safety layer that is wrapped around the RL agent. It extracts all necessary constraints from the power system model and checks whether a control action suggested by the agent is safe. If necessary, the safety layer adjust the action, before passing it on to the simulation. The agent then receives a feedback informing it about the adjustment of its action.\n",
    "\n",
    "Within this notebook, you will learn how to \n",
    "- use CommonPower to modularly construct a power system,\n",
    "- set up an RL agent,\n",
    "- assign nodes to this agent, \n",
    "- train the RL agent, and\n",
    "- monitor the training process using Tensorboard.\n",
    "\n",
    "## Before getting started\n",
    "1. Make sure you install all necessary requirements following the `Readme.txt`\n",
    "2. Optional (only if you want to experiment with tracking training using Weights&Biases): Sign up for the academic version of Weights&Biases [here](https://wandb.ai/site/research).\n",
    "\n",
    "## Important ressources for further information\n",
    "### Short introduction to RL\n",
    "If you have never worked with RL before, we recommend reading the [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) introduction of RL.\n",
    "### PPO implementation\n",
    "We use the RL algorithm implementations from the StableBaselines3 (SB3) repository. You can learn more about the repository and the available algorithms [here](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html).\n",
    "### Tensorboard\n",
    "[Tensorboard](https://www.tensorflow.org/tensorboard/get_started) can be used to track training of any kind of network.\n",
    "### Weights&Biases\n",
    "Weights&Biases (W&B) is an alternative to Tensorboard with very nice visualizations and some advanced options. It helps you keep an overview of your experiments and compare different hyperparameter settings. Find more information in their [documentation](https://docs.wandb.ai/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from commonpower.modelling import ModelHistory\n",
    "from commonpower.core import System, Node, Bus\n",
    "from commonpower.models.busses import *\n",
    "from commonpower.models.components import *\n",
    "from commonpower.models.powerflow import *\n",
    "from commonpower.control.controllers import RLControllerSB3, OptimalController\n",
    "from commonpower.control.safety_layer.safety_layers import ActionProjectionSafetyLayer\n",
    "from commonpower.control.runners import SingleAgentTrainer, DeploymentRunner\n",
    "from commonpower.control.wrappers import SingleAgentWrapper\n",
    "from commonpower.control.logging.loggers import *\n",
    "from commonpower.data_forecasting import *\n",
    "from commonpower.utils.param_initialization import *\n",
    "from stable_baselines3 import PPO\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64881e9d",
   "metadata": {},
   "source": [
    "## System set-up\n",
    "\n",
    "First, we have to define the power system within which we want to control one node using an RL agent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = timedelta(hours=24)\n",
    "frequency = timedelta(minutes=60)\n",
    "fixed_start = \"27.11.2016\"\n",
    "\n",
    "# path to data profiles\n",
    "current_path = pathlib.Path().absolute()\n",
    "data_path = current_path / 'data' / '1-LV-rural2--1-sw'\n",
    "data_path = data_path.resolve()\n",
    "\n",
    "ds1 = CSVDataSource(data_path  / 'LoadProfile.csv',\n",
    "            delimiter=\";\", \n",
    "            datetime_format=\"%d.%m.%Y %H:%M\", \n",
    "            rename_dict={\"time\": \"t\", \"H0-A_pload\": \"p\", \"H0-A_qload\": \"q\"},\n",
    "            auto_drop=True, \n",
    "            resample=timedelta(minutes=60))\n",
    "\n",
    "ds2 = CSVDataSource(data_path / 'LoadProfile.csv',\n",
    "            delimiter=\";\", \n",
    "            datetime_format=\"%d.%m.%Y %H:%M\", \n",
    "            rename_dict={\"time\": \"t\", \"G1-B_pload\": \"psib\", \"G1-C_pload\": \"psis\", \"G2-A_pload\": \"psi\"},\n",
    "            auto_drop=True, \n",
    "            resample=timedelta(minutes=60))\n",
    "\n",
    "ds3 = CSVDataSource(data_path / 'RESProfile.csv', \n",
    "        delimiter=\";\", \n",
    "        datetime_format=\"%d.%m.%Y %H:%M\", \n",
    "        rename_dict={\"time\": \"t\", \"PV3\": \"p\"},\n",
    "        auto_drop=True, \n",
    "        resample=timedelta(minutes=60)).apply_to_column(\"p\", lambda x: -x)\n",
    "\n",
    "dp1 = DataProvider(ds1, LookBackForecaster(frequency=frequency, horizon=horizon))\n",
    "dp2 = DataProvider(ds2, LookBackForecaster(frequency=frequency, horizon=horizon))\n",
    "dp3 = DataProvider(ds3, PerfectKnowledgeForecaster(frequency=frequency, horizon=horizon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes\n",
    "n1 = Bus(\"MultiFamilyHouse\", {\n",
    "    'p': (-50, 50),\n",
    "    'q': (-50, 50),\n",
    "    'v': (0.95, 1.05),\n",
    "    'd': (-15, 15)\n",
    "})\n",
    "\n",
    "# trading unit with price data for buying and selling electricity (to reduce problem complexity, we assume that\n",
    "# prices for selling and buying are the same --> TradingLinear)\n",
    "m1 = TradingBusLinear(\"Trading1\", {\n",
    "    'p': (-50, 50),\n",
    "    'q': (-50, 50)\n",
    "}).add_data_provider(dp2)\n",
    "\n",
    "# components\n",
    "# energy storage sytem\n",
    "capacity = 3  #kWh\n",
    "e1 = ESSLinear(\"ESS1\", {\n",
    "    'rho': 0.1, \n",
    "    'p': (-1.5, 1.5), \n",
    "    'q': (0, 0), \n",
    "    'soc': (0.2 * capacity, 0.8 * capacity), \n",
    "    \"soc_init\": RangeInitializer(0.2 * capacity, 0.8 * capacity)\n",
    "})\n",
    "\n",
    "# photovoltaic with generation data\n",
    "r1 = RenewableGen(\"PV1\").add_data_provider(dp3)\n",
    "\n",
    "# static load with data source\n",
    "d1 = Load(\"Load1\").add_data_provider(dp1)\n",
    "\n",
    "# we first have to add the nodes to the system \n",
    "# and then add components to the node in order to obtain a tree-like structure\n",
    "sys = System(power_flow_model=PowerBalanceModel()).add_node(n1).add_node(m1)\n",
    "\n",
    "# add components to nodes\n",
    "n1.add_node(d1).add_node(e1).add_node(r1)\n",
    "\n",
    "# show system structure: \n",
    "sys.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321df867",
   "metadata": {},
   "source": [
    "## Setting up the RL Controller\n",
    "\n",
    "We first set up a controller, then add the node we want to controll. The system will be balanced through the market node, which is controlled by an optimal controller (handed over as `global_controller` when instantiating the `SingleAgentTrainer`).\n",
    "\n",
    "Since RL controllers do not naturally allow considering constraints (such as a limit on the state of charge of the storage system), we have to add a safety layer to the controller. The `ActionProjectionSafetyLayer` outputs an action that is as close as possible to the action suggested by the RL controller while also satisfying all constraints of the system. Every time the safety layer has to intervene, a penalty term is added to the reward of the RL agent. A `penalty_factor` is used to weigh this penalty and the rest of the reward. \n",
    "\n",
    "Furthermore, the node will have to buy electricity to even out its power balance. To inform the controller of the cost of electricity, we use the `price_callback` function which is linked to the market node controlled by the global controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcff381",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = RLControllerSB3(\n",
    "    name='agent1', \n",
    "    safety_layer=ActionProjectionSafetyLayer(penalty_factor=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b837fb",
   "metadata": {},
   "source": [
    "We use the SB3 PPO implementation to train our RL agent and log the training progress using Tensorboard. If you want to try Weights&Biases for logging, you can uncomment the respective line. For more information on potential hyperparameters for PPO, check the [documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35caa16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a seed for the random number generator used during training (It is common to train with ~5 different\n",
    "# random seeds when you are, for example, testing a new safeguarding approach. For this notebook, one seed is enough.\n",
    "# It will improve reproducibility of results.)\n",
    "training_seed = 42\n",
    "\n",
    "# set up configuration for the PPO algorithm\n",
    "alg_config = {}\n",
    "alg_config['total_steps'] = 1500*int(horizon.total_seconds() // 3600)\n",
    "alg_config['algorithm'] = PPO\n",
    "alg_config['policy'] = 'MlpPolicy'\n",
    "alg_config['device'] = 'cpu'\n",
    "alg_config['n_steps'] = int(horizon.total_seconds() // 3600)\n",
    "alg_config['learning_rate'] = 0.0008\n",
    "alg_config['batch_size'] = 12\n",
    "\n",
    "# set up logger\n",
    "log_dir = './test_run/'\n",
    "logger = TensorboardLogger(log_dir='./test_run/')\n",
    "# You can also use Weights&Biases to monitor training. If you uncomment the next line, make sure to exchange the \n",
    "# \"entity_name\" parameter!\n",
    "# logger = WandBLogger(log_dir='./test_run/', entity_name=\"srl4ps\", project_name=\"commonpower\", alg_config=alg_config, callback=WandBSafetyCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707b80c",
   "metadata": {},
   "source": [
    "## Running the Training\n",
    "\n",
    "To run the training, we first need to instantiate a runner object. The `SingleAgentWrapper` is used to make the system compatible with the stable baselines PPO implementation. WARNING: This will take a while (~two hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae8a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify the path where the model should be saved\n",
    "model_path = \"./saved_models/my_model\"\n",
    "\n",
    "runner = SingleAgentTrainer(\n",
    "    sys=sys, \n",
    "    global_controller=agent1, \n",
    "    wrapper=SingleAgentWrapper, \n",
    "    alg_config=alg_config, \n",
    "    forecast_horizon=horizon,\n",
    "    control_horizon=horizon,\n",
    "    logger = logger,\n",
    "    save_path = model_path, \n",
    "    seed = training_seed\n",
    ")\n",
    "runner.run(fixed_start=fixed_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6176d",
   "metadata": {},
   "source": [
    "### Training visualization\n",
    "If you used the TensorBoardLogger, you can plot the training metrics using the notebook magic of tensorboard. The most interesting charts for us are the `safety/mean_eps_penalty`, `safety/n_action_correction`, `rollout/ep_reward_mean`, `train/loss`, and `train/explained_variance`. Think about what these charts tell you and discuss it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876257bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir test_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b1d86",
   "metadata": {},
   "source": [
    "## Deploying the trained agent\n",
    "After training the agent, you can deploy it on the system. This means that the neural network representing our controller will deterministically chose the best control input for the given observation according to its policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33336015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for demonstration purposes, we show here how to load a pre-trained policy\n",
    "# However, in the present case this would not be necessary, since \"agent1\" has saved the policy after training\n",
    "\n",
    "# First, we need to create a new agent and pass the pretrained_policy_path from which to load the neural network \n",
    "# params. Adding n1 to this agent will create a warning since we are overwriting agent1, which is desired in this case\n",
    "agent2 = RLControllerSB3(\n",
    "    name=\"pretrained_agent\", \n",
    "    safety_layer=ActionProjectionSafetyLayer(penalty_factor=0.1),\n",
    "    pretrained_policy_path = model_path\n",
    ")\n",
    "\n",
    "# The deployment runner has to be instantiated with the same arguments used during training\n",
    "# The runner will automatically recognize that it has to load the policy for agent2\n",
    "# To ensure proper comparison of the trained RL agent with an optimal controller, we use the same seed for both\n",
    "eval_seed = 5\n",
    "\n",
    "rl_model_history = ModelHistory([sys])\n",
    "rl_deployer = DeploymentRunner(\n",
    "    sys=sys, \n",
    "    global_controller=agent2,  \n",
    "    alg_config=alg_config,\n",
    "    wrapper=SingleAgentWrapper,\n",
    "    forecast_horizon=horizon,\n",
    "    control_horizon=horizon,\n",
    "    history=rl_model_history,\n",
    "    seed = eval_seed\n",
    ")\n",
    "# Finally, we can simulate the system with the trained controller for the given day\n",
    "rl_deployer.run(n_steps=24, fixed_start=fixed_start)\n",
    "# let us extract some logs for comparison with an optimal controller\n",
    "# We want to compare the cost of the household over the curse of the day. \n",
    "rl_power_import_cost = rl_model_history.get_history_for_element(m1, name='cost') # cost for buying electricity\n",
    "rl_dispatch_cost = rl_model_history.get_history_for_element(n1, name='cost') # cost for operating the components in the household\n",
    "rl_total_cost = [(rl_power_import_cost[t][0], rl_power_import_cost[t][1] + rl_dispatch_cost[t][1]) for t in range(len(rl_power_import_cost))]\n",
    "rl_soc = rl_model_history.get_history_for_element(e1, name=\"soc\") # state of charge of the battery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f330e",
   "metadata": {},
   "source": [
    "## Benchmarking Trained Agent and Optimal Controller\n",
    "We want to compare the results of our trained agent with an optimal controller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the same system but we have to set up a new runner. \n",
    "# This time, the global controller will take over the control of the household\n",
    "oc_model_history = ModelHistory([sys])\n",
    "oc_deployer = DeploymentRunner(\n",
    "    sys=sys, \n",
    "    global_controller=OptimalController('global'), \n",
    "    forecast_horizon=horizon,\n",
    "    control_horizon=horizon,\n",
    "    history=oc_model_history,\n",
    "    seed = eval_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e35735",
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_deployer.run(n_steps=24, fixed_start=fixed_start)\n",
    "# we retrieve logs for the system cost\n",
    "oc_power_import_cost = oc_model_history.get_history_for_element(m1, name='cost') # cost for buying electricity\n",
    "oc_dispatch_cost = oc_model_history.get_history_for_element(n1, name='cost') # cost for operating the components in the household\n",
    "oc_total_cost = [(oc_power_import_cost[t][0], oc_power_import_cost[t][1] + oc_dispatch_cost[t][1]) for t in range(len(oc_power_import_cost))]\n",
    "oc_soc = oc_model_history.get_history_for_element(e1, name=\"soc\") # state of charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79752f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the cost of RL agent and optimal controller\n",
    "plt.plot(range(len(rl_total_cost)), [x[1] for x in rl_total_cost], label=\"Cost RL\")\n",
    "plt.plot(range(len(oc_total_cost)), [x[1] for x in oc_total_cost], label=\"Cost optimal control\")\n",
    "plt.xticks(ticks=range(len(rl_power_import_cost)), labels=[x[0] for x in rl_power_import_cost])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Comparison of household cost for RL and optimal controller\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc81b6",
   "metadata": {},
   "source": [
    "To make sure that both runs are comparable, we check that they started with the same initial SOC of the battery.\n",
    "It is the only random element in the current system set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c85823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the state of charge of the batteries\n",
    "plt.plot(range(len(rl_soc)), [x[1] for x in rl_soc], label=\"SOC RL\")\n",
    "plt.plot(range(len(oc_soc)), [x[1] for x in oc_soc], label=\"SOC optimal control\")\n",
    "plt.xticks(ticks=range(len(rl_soc)), labels=[x[0] for x in rl_soc])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Comparison of battery state of charge (SOC) for RL and optimal controller\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the total cost for one day:\n",
    "cost_day_rl = sum([rl_total_cost[t][1] for t in range(len(rl_total_cost))])\n",
    "cost_day_oc = sum([oc_total_cost[t][1] for t in range(len(oc_total_cost))])\n",
    "print(f\"The daily cost \\n a) with the RL controller: {cost_day_rl} \\n b) with the optimal controller: {cost_day_oc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ecc3c",
   "metadata": {},
   "source": [
    "As you can see, the RL controller does not quite achieve the performance of the optimal controller. Why might that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d720ae2",
   "metadata": {},
   "source": [
    "## Things to try\n",
    "\n",
    "You can use this notebook to experiment a bit. Here are some ideas:\n",
    "- Try changing the `penalty_factor` and see how it affects the training\n",
    "- Try setting the `fixed_start` argument in `runner.run()` to `None` to train on multiple days from one year. WARNING: You will also have to increase the `total_steps` and probably do some hyper parameter tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38552bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
